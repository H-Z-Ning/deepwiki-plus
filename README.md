# DeepWiki-Plus ğŸš€  
> Elevate any repoâ€™s doc experience from â€œusableâ€ to â€œinstantly clearâ€

DeepWiki-Plus is a **code-level, deeply-enhanced** evolution of the original DeepWiki.  
We kept the beloved â€œone-click beautiful Wikiâ€ super-power, then inserted two **pre-parsing** and **pre-summarization** stages **before** the LLM ever sees the code.  
The result: higher accuracy, better readability, and lower maintenance for every page we generate.

[English](./README.md) | [ç®€ä½“ä¸­æ–‡](./README.zh.md) 
---

## âœ¨ Core Improvements (Exclusive)

| Step | Original DeepWiki | DeepWiki-Plus |
|---|---|---|
| â‘  Code fetch | plain clone | clone + **lightweight static analysis** |
| â‘¡ Semantic extraction | none | **code parsing**: classes, signatures, returns, imports, call chains |
| â‘¢ Pre-summary | none | **LLM second-pass**: module duty + call-graph (Mermaid) |
| â‘£ Wiki generation | raw code context only | **pre-summary injected** â†’ LLM answers â†‘ accuracy, â†“ hallucination |
| â‘¤ Q&A / deep-dive | RAG via Embedding only | RAG **+** pre-summary **dual retrieval**â€”complex call-chain questions answered instantly |

---

## ğŸ¯ New Super-powers

1. **Call-Mapâ„¢** â€“ one-click call-chain graph  
   Every module opens with an auto-generated Mermaid diagram; click any node to jump to the exact source line.

2. **Smart-TOCâ„¢** â€“ intelligent outline  
   Built from pre-summarized â€œmodule dutiesâ€ so the wiki never shows empty sections or â€œmiscâ€ dumps.

3. **Private-Graphâ„¢** â€“ internal-only highlights  
   For private repos we redact external deps and keep only in-house call relationsâ€”compliance-safe.

4. **Incremental refresh**  
   Later just `git pull` the delta; parsing, summarization and embedding are all incrementalâ€”**80 % time saved**.

---

## ğŸš€ 30-second Quick-start (Alibaba Qwen example)

```bash
# 1. Clone DeepWiki-Plus
git clone https://github.com/H-Z-Ning/deepwiki-plus.git && cd deepwiki-plus

# 2. Add keys (multi-model support)
vi .env

GOOGLE_API_KEY=your_key
OPENAI_API_KEY=your_key
OPENAI_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1 
OPENAI_MODEL=qwen-turbo

# 3. Replace api/config/embedder.json with the content of api/config/embedder_openai_compatible.json

# 4. One-liner launch (pre-parser included)
npm run dev          # frontend
python -m api.main   # backend + pre-parser worker

# 5. Frontend model setup
   (1) Provider: OpenAI  
   (2) Check â€œCustom modelâ€  
   (3) Model name: qwen-turbo
```

Open http://localhost:3000  
Paste any repo URL â†’ tick **â€œEnable deep pre-parseâ€** â†’ in 1-3 min enjoy a **call-chain-powered** wiki!

---

## ğŸ› ï¸ Architecture Upgrade

```
deepwiki/
â”œâ”€â”€ api/                          # backend
â”‚   â”œâ”€â”€ main.py                   # API entry
â”‚   â”œâ”€â”€ api.py                    # FastAPI impl
â”‚   â”œâ”€â”€ rag.py                    # retrieval-augmented generation
â”‚   â”œâ”€â”€ data_pipeline.py          # data utils
â”‚   â”œâ”€â”€ tools/project_parser.py   # NEW: pre-parser
â”‚   â”œâ”€â”€ websocket_wiki.py         # NEW: LLM pre-summarization service
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ src/                          # Next.js frontend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ page.tsx              # landing
â”‚   â”‚   â””â”€â”€ [owner]/[repo]/page.tsx   # NEW: wiki with call-map + summary
â”‚   â””â”€â”€ components/
â”‚       â””â”€â”€ Mermaid.tsx           # Mermaid renderer
â”œâ”€â”€ public/
â”œâ”€â”€ package.json
â””â”€â”€ .env                          # create from template
```

---

## ğŸ” How It Works (Upgraded Flow)

```mermaid
graph TD
    A[Repo URL] -->|clone| B[Code pre-parser]
    B --> C[Extract classes / funcs / imports / call chains]
    C --> D[LLM pre-summary: duty + call graph]
    D --> E[Generate Call-Mapâ„¢]
    E --> F[Store: parse snapshot + summary]
    F --> G[Standard RAG pipeline]
    G --> H[Wiki / Q&A / deep research]
    style B fill:#f9f,stroke:#333
    style D fill:#bbf,stroke:#333
```

---

## ğŸ§ª Advanced Tricks

- **Local models**  
  Set `OLLAMA_HOST` and run fully offline; pre-parser supports local CodeLlama-34B out-of-the-box.

- **Custom parsing rules**  
  Drop `.yaml` files in `api/parser/rules/` to support private frameworks or DSLs.

- **CI integration**  
  Official GitHub Action supplied:  
  `.github/workflows/deepwiki-plus.yml` â€” auto incremental Wiki update on every push.

---

## ğŸ¤ Contribute & Feedback

We love PRs that keep docs from collecting dust:

- New language parsers (Java, Rust, Go, Zig â€¦)  
- Better call-chain layout algorithms  
- Shinier Mermaid themes

ğŸ‘‰ [Discord](https://discord.com/invite/VQMBGR8u5v)  
ğŸ‘‰ [Issue template](https://github.com/H-Z-Ning/deepwiki-plus/issues)

---

## ğŸ“„ License

MIT Â© DeepWiki-Plus Contributors  
â€œStanding on the shoulders of giantsâ€”and taking one more step.â€

---

### ğŸ§© Using OpenAI-Compatible Embedding Models (e.g. Alibaba Qwen)

To switch to an OpenAI-compatible embedding service (such as Alibabaâ€™s Qwen):

1. Overwrite `api/config/embedder.json` with the contents of `api/config/embedder_openai_compatible.json`.
2. In the `.env` file at project root, add:
   ```
   GOOGLE_API_KEY=your_key
   OPENAI_API_KEY=your_key
   OPENAI_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1 
   OPENAI_MODEL=qwen-turbo
   ```
3. In the web UI:  
   (1) Provider: OpenAI  
   (2) Check â€œCustom modelâ€  
   (3) Model name: qwen-turbo  
4. Environment variables are automatically substituted into embedder.json at runtimeâ€”zero code changes required.
